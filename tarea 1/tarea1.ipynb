{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1: DS Lab\n",
    "- Ricardo Arancibia\n",
    "- José Díaz\n",
    "- Javier Santibáñez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinplot(flip=1):\n",
    "    x = np.linspace(0, 14, 100)\n",
    "    for i in range(1, 7):\n",
    "        plt.plot(x, np.sin(x + i * .5) * (7 - i) * flip)\n",
    "sinplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estilo(sty = \"whitegrid\"):\n",
    "    param = {'axes.facecolor': 'white',\n",
    "             'axes.edgecolor': 'black',\n",
    "             'axes.grid': True,\n",
    "             'axes.axisbelow': 'line',\n",
    "             'axes.labelcolor': 'black',\n",
    "             'figure.facecolor': (1, 1, 1, 0),\n",
    "             'grid.color': '#b0b0b0',\n",
    "             'grid.linestyle': '-',\n",
    "             'text.color': 'black',\n",
    "             'xtick.color': 'red',\n",
    "             'ytick.color': 'black',\n",
    "             'xtick.direction': 'out',\n",
    "             'ytick.direction': 'out',\n",
    "             'lines.solid_capstyle': 'projecting',\n",
    "             'patch.edgecolor': 'black',\n",
    "             'image.cmap': 'viridis',\n",
    "             'font.family': ['sans-serif'],\n",
    "             'font.sans-serif': ['DejaVu Sans',\n",
    "             'Bitstream Vera Sans',\n",
    "             'Computer Modern Sans Serif',\n",
    "             'Lucida Grande',\n",
    "             'Verdana',\n",
    "             'Geneva',\n",
    "             'Lucid',\n",
    "             'Arial',\n",
    "             'Helvetica',\n",
    "             'Avant Garde',\n",
    "             'sans-serif'],\n",
    "             'patch.force_edgecolor': False,\n",
    "             'xtick.bottom': True,\n",
    "             'xtick.top': False,\n",
    "             'ytick.left': True,\n",
    "             'ytick.right': False,\n",
    "             'axes.spines.left': True,\n",
    "             'axes.spines.bottom': True,\n",
    "             'axes.spines.right': False,\n",
    "             'axes.spines.top': False}\n",
    "    sns.set_style(sty, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estilo()\n",
    "sinplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "class RegresionBayesianaEmpirica(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self, alpha_0, beta_0, tol=1e-5, maxiter=200):\n",
    "        self.set_params(alpha_0=alpha_0,beta_0=beta_0,tol=tol,maxiter=maxiter)\n",
    "\n",
    "    def get_posteriori(self, X, y, alpha, beta):\n",
    "        N,d = X.shape\n",
    "        S_N_1 = alpha*np.identity(d) + beta*(X.T @ X)\n",
    "        S_N = np.linalg.inv(S_N_1)\n",
    "        m_N = beta*((S_N @ X.T) @ y)\n",
    "        return m_N, S_N\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N,d = X.shape\n",
    "        alpha_old = self.alpha_0\n",
    "        beta_old = self.beta_0\n",
    "        print('Iteracion: 1')\n",
    "        print('valores actuales, alpha:{}, beta:{}'.format(alpha_old,beta_old))\n",
    "        lambdas, vectores = np.linalg.eig(beta_old*(X.T @ X))\n",
    "        lambdas = np.real_if_close(lambdas,tol=1)\n",
    "        gamma = np.sum(lambdas/(alpha_old + lambdas))\n",
    "        m_N, S_N = self.get_posteriori(X,y,alpha_old,beta_old)\n",
    "        alpha_new = gamma/(m_N.T @ m_N)\n",
    "        beta_new = (N-gamma)/(np.sum((y-m_N.T@X.T)**2))\n",
    "        for k in range(2, self.maxiter+1): \n",
    "            err_alpha = abs(alpha_old - alpha_new)\n",
    "            err_beta = abs(beta_old - beta_new)\n",
    "            if (err_alpha < self.tol) and (err_beta < self.tol):\n",
    "                break\n",
    "            else:\n",
    "                print('Iteracion: ', k)\n",
    "                print('valores actuales, alpha:{}, beta:{}'.format(alpha_new,beta_new))\n",
    "                alpha_old = alpha_new\n",
    "                beta_old = beta_new\n",
    "                lambdas, vectores = np.linalg.eig(beta_old*(X.T @ X))\n",
    "                lambdas = np.real_if_close(lambdas,tol=1)\n",
    "                gamma = np.sum(lambdas/(alpha_old + lambdas))\n",
    "                m_N, S_N = self.get_posteriori(X,y,alpha_old,beta_old)\n",
    "                alpha_new = gamma/(m_N.T @ m_N)\n",
    "                beta_new = (N-gamma)/(np.sum((y-m_N.T@X.T)**2))\n",
    "        self.alpha_0 = alpha_new\n",
    "        self.beta_0 = beta_new\n",
    "        print('\\nvalores finales, alpha:{}, beta:{}'.format(alpha_new,beta_new))\n",
    "        m_N, S_N = self.get_posteriori(X,y,alpha_new,beta_new)\n",
    "        self.m_N = m_N\n",
    "        self.S_N = S_N\n",
    "\n",
    "    def predict(self, X_, return_std=False):\n",
    "        y_ = X_ @ self.m_N\n",
    "        y_std = np.sqrt(1/self.beta_0 + np.diag((X_ @ self.S_N) @ X_.T))\n",
    "        if return_std:\n",
    "            return y_, y_std\n",
    "        else:\n",
    "            return y_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplito del fit\n",
    "mat = np.array([[1, 0, 0], [0, 1, 0], [0,0,1], [0,0,0]])\n",
    "mat_y = np.array([10, 12,7,5])\n",
    "rbe = RegresionBayesianaEmpirica(0.5, 2/3)\n",
    "rbe.fit(mat, mat_y)\n",
    "print('prediccion: ',rbe.predict(np.array([[1,2,3]]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1    6186\n2    5070\n0    1682\n3     692\n4     483\n5       5\nName: n_garajes, dtype: int64"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "data['n_garajes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(14118, 18)\n"
    }
   ],
   "source": [
    "## JS: Hay que definir las columnas numericas y categoricas\n",
    "# data.columns\n",
    "cat_cols = ['tipo_producto', 'rent_type', 'location','furnished','Cluster']\n",
    "\n",
    "num_perc_cols = ['n_rooms', 'n_bath','n_garajes', 'metrocuadrado_index',\n",
    "       'trabajoinf_ninos_5_17_anos_perc',\n",
    "       'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "       'jovenes_14_24_anos_nini_perc',\n",
    "       'indice_envegecimiento',\n",
    "       'jefe_mujer_perc',\n",
    "       'adultos_mayores_pobres_perc',\n",
    "       'indice_inseguridad', \n",
    "       'areas_verdes_perc']\n",
    "\n",
    "num_cols =['Densidad']\n",
    "\n",
    "df_X = data[cat_cols + num_perc_cols + num_cols]\n",
    "print(df_X.shape)\n",
    "df_y = data['price_by_m2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical pipeline\n",
    "cat_pipe = Pipeline([('encoding', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# numerical pipeline\n",
    "num_perc_pipe = Pipeline([('scaler', MinMaxScaler()), ('poly', PolynomialFeatures(degree=3))])\n",
    "num_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=3))])\n",
    "\n",
    "prep = ColumnTransformer(transformers=[('num_perc', num_perc_pipe, num_perc_cols),('num', num_pipe, num_cols), ('cat', cat_pipe, cat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pipe_run(X, y, pipeline):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    RMSE = np.sqrt(np.mean((y_pred - y_test)**2))\n",
    "    R2 = pipeline.score(X_test, y_test)\n",
    "    print('RMSE: ', RMSE)\n",
    "    print('R^2: ', R2)\n",
    "    return pipeline, RMSE, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Probar varios alpha y beta\n",
    "alphas = np.linspace(1e-10, 1e-5, 3, endpoint=True)\n",
    "betas = np.linspace(1e-10, 1e-5, 3, endpoint=True)\n",
    "params = []\n",
    "R2s = []\n",
    "\n",
    "for alpha in [1e-9]:#alphas:\n",
    "    for beta in [1e-9]:#betas:\n",
    "        el_pipe = Pipeline([('prep', prep), ('regressor', RegresionBayesianaEmpirica(alpha,beta,tol=1e-15))])\n",
    "        regressor, RMSE, R2 = pipe_run(df_X, df_y, el_pipe)\n",
    "        params.append((alpha,beta))\n",
    "        R2s.append(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_opt, beta_opt = params[R2s.index(max(R2s))]\n",
    "print('Modelo Optimo')\n",
    "print('alpha:{}, beta:{}'.format(alpha_opt, beta_opt))\n",
    "el_pipe = Pipeline([('prep', prep), ('regressor', RegresionBayesianaEmpirica(alpha_opt,beta_opt))])\n",
    "regressor, RMSE, R2 = pipe_run(df_X, df_y, el_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "## JS: Hay que REdefinir las columnas numericas y categoricas... obtenidas de EDA\n",
    "cat_cols = ['tipo_producto', 'Cluster', 'rent_type', 'location','furnished']\n",
    "\n",
    "num_perc_cols = ['n_rooms', 'n_bath','n_garajes', 'metrocuadrado_index',\n",
    "       'trabajoinf_ninos_5_17_anos_perc',\n",
    "       'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "       'jovenes_14_24_anos_nini_perc',\n",
    "       'indice_envegecimiento',\n",
    "       'jefe_mujer_perc',\n",
    "       'adultos_mayores_pobres_perc',\n",
    "       'indice_inseguridad', \n",
    "       'areas_verdes_perc']\n",
    "\n",
    "num_cols =['Densidad']\n",
    "\n",
    "df_X = data[cat_cols + num_perc_cols + num_cols]\n",
    "# print(df_X.shape)\n",
    "df_y = data['price_by_m2']\n",
    "\n",
    "prep = ColumnTransformer(transformers=[('num_perc', num_perc_pipe, num_perc_cols),('num', num_pipe, num_cols), ('cat', cat_pipe, cat_cols)])\n",
    "\n",
    "el_pipe = Pipeline([('prep', prep), ('regressor', RegresionBayesianaEmpirica(alpha_opt,beta_opt))])\n",
    "regressor, RMSE, R2 = pipe_run(df_X, df_y, el_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# guardar modelo\n",
    "filename = 'modelo.pkl'\n",
    "pickle.dump(regressor, open(filename, 'wb'))\n",
    " \n",
    "# para cargar modelo:\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "ridge_pipe = Pipeline([('prep', prep), ('regressor_ridge', BayesianRidge())])\n",
    "regressor_ridge, RMSE_ridge, R2_ridge = pipe_run(df_X, df_y, ridge_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}